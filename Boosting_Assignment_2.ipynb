{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c3ada7-40c4-4de0-9abc-bda9be1f7133",
   "metadata": {},
   "source": [
    "# ANSWER 1\n",
    "Gradient Boosting Regression is a machine learning technique that uses the gradient boosting framework to solve regression problems. It is an extension of the gradient boosting algorithm, which was originally designed for classification tasks, to handle continuous target variables and make accurate predictions for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420817ec-a879-4d23-b0c7-5a65d5ec8fc2",
   "metadata": {},
   "source": [
    "# ANSWER 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e419e35a-7e9b-4a66-9ff7-5c63963da634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from random import randint, uniform\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d325fa85-a5c9-4734-b496-200a4fd04b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the y_pred using the mean of the target values\n",
    "        y_pred = np.full(len(y), np.mean(y))\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate the negative gradient for the current estimator\n",
    "            gradient = y - y_pred\n",
    "\n",
    "            # Fit a weak learner (in this case, a decision stump) to the negative gradient (residuals)\n",
    "            stump_pred = X[:, 0] < 0.5  # Split the data at the midpoint\n",
    "            residual_stump = gradient - np.mean(gradient[stump_pred])\n",
    "\n",
    "            # Update the y_pred with a fraction of the predictions to prevent overfitting\n",
    "            y_pred += self.learning_rate * np.dot(X[:, 0] < 0.5, residual_stump) / np.sum(X[:, 0] < 0.5)\n",
    "\n",
    "            self.estimators.append((np.mean(gradient[stump_pred]), X[:, 0] < 0.5))\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for stump_val, stump_pred in self.estimators:\n",
    "            y_pred += self.learning_rate * np.dot(X[:, 0] < 0.5, stump_val)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6009ff93-7192-4105-aa3e-178d8d065bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8385341-0db0-4ca7-8ce5-34394d336f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 11973.660335875124\n",
      "R-squared: -6.191493235300815\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56835749-a76d-4650-b71d-516222ca3340",
   "metadata": {},
   "source": [
    "# ANSWER 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f457753-8fba-4510-9a64-c6a4a8408040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from scipy.stats import uniform, randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b61b864-6e3c-4bf0-b318-fcfac162c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fafbbcd-02f4-4fa1-8587-8c098da803e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3, estimator=GradientBoostingRegressor(),\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7fdf0eb87760&gt;,\n",
       "                                        &#x27;max_depth&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7fdf0eb87220&gt;,\n",
       "                                        &#x27;n_estimators&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7fdf0e94dc00&gt;},\n",
       "                   random_state=42, scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3, estimator=GradientBoostingRegressor(),\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7fdf0eb87760&gt;,\n",
       "                                        &#x27;max_depth&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7fdf0eb87220&gt;,\n",
       "                                        &#x27;n_estimators&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7fdf0e94dc00&gt;},\n",
       "                   random_state=42, scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=GradientBoostingRegressor(),\n",
       "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7fdf0eb87760>,\n",
       "                                        'max_depth': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7fdf0eb87220>,\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7fdf0e94dc00>},\n",
       "                   random_state=42, scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(2, 8),\n",
    "}\n",
    "\n",
    "Gradient_Boosting_Regressor = GradientBoostingRegressor()\n",
    "\n",
    "random_search = RandomizedSearchCV(Gradient_Boosting_Regressor, param_distributions=param_dist, n_iter=10, scoring='neg_mean_squared_error', cv=3, random_state=42)\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5e34730-d4ff-438f-8993-38f57be76a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de1f7df8-419f-4f40-af7e-73f84067b76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.11495493205167782, 'max_depth': 3, 'n_estimators': 64}\n",
      "Mean Squared Error: 160.0893826801497\n",
      "R-squared: 0.9038488081093845\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa01efe-e6fc-45f3-82f3-c6c4f97be91e",
   "metadata": {},
   "source": [
    "# ANSWER 4\n",
    "In the context of Gradient Boosting, a weak learner refers to a simple model that performs slightly better than random guessing on a given learning task. Weak learners are typically simple, low-complexity models, such as decision trees with limited depth (often called decision stumps), linear models, or shallow neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7957fd-3717-47ee-aa56-39a894e773c4",
   "metadata": {},
   "source": [
    "# ANSWER 5\n",
    "The intuition behind the Gradient Boosting algorithm can be understood as follows:\n",
    "1. Ensemble Learning: Gradient Boosting is an ensemble learning technique, which means it combines the predictions of multiple weak learners (e.g., decision trees) to create a more accurate and robust model. Each weak learner contributes its predictions, and their collective knowledge is used to make the final prediction.\n",
    "2. Sequential Improvement: Gradient Boosting builds the ensemble of weak learners in a sequential and iterative manner. It starts with an initial prediction (usually a simple estimate like the mean for regression or the majority class for classification) and then continuously improves it in each iteration.\n",
    "3. Correcting Errors: The core idea of Gradient Boosting is to correct the errors made by the previous weak learners. In each iteration, a new weak learner is trained to focus on the remaining errors or residuals of the current ensemble.\n",
    "4. Gradient Descent-like Optimization: In the training process, the weak learners are fitted to the negative gradient of the loss function with respect to the current ensemble's predictions. This approach is similar to gradient descent, where the weak learners aim to minimize the loss function by updating their parameters.\n",
    "4. Emphasis on Difficult Examples: By focusing on the residuals (errors) in the training data, Gradient Boosting pays more attention to the difficult-to-predict examples. The subsequent weak learners learn to capture the patterns that are challenging for the current ensemble, leading to improved model performance.\n",
    "5. Weighted Voting: The predictions of the weak learners are combined using weighted voting, where the predictions of more accurate weak learners are given higher importance. The combination of predictions from multiple learners results in a strong model capable of capturing complex relationships in the data.\n",
    "6. Regularization: Gradient Boosting can be prone to overfitting, especially if the number of iterations (weak learners) is too high. To address this, techniques like shrinkage (learning rate) and subsampling can be used to regularize the boosting process and prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e707b-d5bb-4f72-b255-c47161064d76",
   "metadata": {},
   "source": [
    "# ANSWER 6\n",
    "Steps Gradient Boosting algorithm build an ensemble of weak learners : \n",
    "1. Initialization: The process starts with an initial prediction, which can be a simple estimate like the mean of the target values for regression tasks or the majority class for classification tasks. This initial prediction serves as the starting point for the iterative process.\n",
    "2. Residual Calculation: The differences between the actual target values and the current ensemble's predictions are computed. These differences, known as residuals, represent the errors that need to be corrected in the subsequent iterations.\n",
    "3. Training of Weak Learners: A weak learner (e.g., decision tree with limited depth) is trained to predict the residuals. The weak learner is fitted to the negative gradient of the loss function with respect to the current ensemble's predictions. This step involves gradient descent-like optimization, where the weak learner aims to minimize the loss by updating its parameters.\n",
    "4. Shrinking: The predictions of the weak learner are then multiplied by a learning rate (also known as the shrinkage rate), which is a value less than 1. This step helps control the contribution of each weak learner to the final ensemble. Smaller learning rates lead to a more cautious learning process and can prevent overfitting.\n",
    "5. Updating Ensemble Predictions: The predictions of the weak learner (multiplied by the learning rate) are added to the current ensemble's predictions. This step updates the ensemble's predictions to incorporate the new information from the latest weak learner. The ensemble now provides an improved estimate of the target variable.\n",
    "6. Iterative Process: Steps 2 to 5 are repeated for a predefined number of iterations or until a stopping criterion (e.g., a minimum improvement in the loss function) is met. In each iteration, the weak learner focuses on the remaining errors (residuals) and tries to minimize them.\n",
    "7. Ensemble Prediction: The final prediction of the Gradient Boosting algorithm is the sum of the initial prediction and the predictions from all the weak learners, each multiplied by its corresponding learning rate. The ensemble of weak learners collectively works together to achieve a highly accurate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e16d7b-3316-48b3-a93d-88b29c2df5f0",
   "metadata": {},
   "source": [
    "# ANSWER 7\n",
    "1. Objective Function: The first step is to define the objective function to be optimized during the training process. For regression tasks, the common objective function is the mean squared error (MSE), while for classification tasks, the cross-entropy loss (log loss) is commonly used. These objective functions quantify the difference between the actual target values and the model's predictions.\n",
    "2. Initialization: The process starts with an initial prediction (often a simple estimate like the mean of target values for regression or the log-odds for classification). The initial prediction serves as the starting point for the iterative process.\n",
    "3. Residual Calculation: The differences between the actual target values and the current ensemble's predictions are computed. These differences, known as residuals, represent the errors that need to be corrected in the subsequent iterations. For regression, the residuals are the true target values minus the initial predictions. For classification, the residuals are the true class probabilities (encoded as 0s and 1s) minus the initial predicted probabilities.\n",
    "4. Training of Weak Learners: A weak learner (e.g., decision tree with limited depth) is trained to predict the residuals. The weak learner is fitted to the negative gradient of the objective function with respect to the current ensemble's predictions. This step involves gradient descent-like optimization, where the weak learner aims to minimize the loss by updating its parameters.\n",
    "5. Learning Rate: The predictions of the weak learner are multiplied by a learning rate (also known as the shrinkage rate). The learning rate controls the contribution of each weak learner to the final ensemble. Smaller learning rates lead to a more cautious learning process and can prevent overfitting.\n",
    "6. Updating Ensemble Predictions: The predictions of the weak learner (multiplied by the learning rate) are added to the current ensemble's predictions. This step updates the ensemble's predictions to incorporate the new information from the latest weak learner. The ensemble now provides an improved estimate of the target variable.\n",
    "7. Iterative Process: Steps 3 to 6 are repeated for a predefined number of iterations or until a stopping criterion (e.g., a minimum improvement in the objective function) is met. In each iteration, the weak learner focuses on the remaining errors (residuals) and tries to minimize them.\n",
    "8. Ensemble Prediction: The final prediction of the Gradient Boosting algorithm is the sum of the initial prediction and the predictions from all the weak learners, each multiplied by its corresponding learning rate. The ensemble of weak learners collectively works together to achieve a highly accurate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48276574-4006-40fe-b3d3-12e0518257aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92fd6f6-7913-4b1b-97e7-938e2dd900d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d5ce0-8766-46f2-9273-c3d6d84e7b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
